# Agent Report - Multi-Agent System Architecture & Code Explanation

## ðŸ“‹ Table of Contents

1. [System Overview](#system-overview)
2. [Architecture Design](#architecture-design)
3. [Supervisor Agent](#supervisor-agent)
4. [Image Agent](#image-agent)
5. [Classification Agent](#classification-agent)
6. [Root Cause Agent](#root-cause-agent)
7. [Report Agent](#report-agent)
8. [Orchestrator](#orchestrator)
9. [Code Flow Explanation](#code-flow-explanation)
10. [Advanced Features](#advanced-features)

---

## System Overview

The Wafer Defect Analysis Multi-Agent System uses a **LangGraph-based Supervisor Pattern** to coordinate specialized AI agents. Each agent has a specific responsibility and communicates through a shared state managed by the orchestrator.

### Key Design Principles

1. **Separation of Concerns:** Each agent handles one specific task
2. **State Management:** LangGraph manages state transitions between agents
3. **Intelligent Routing:** Supervisor decides workflow based on analysis results
4. **Graceful Degradation:** System works even if some components fail
5. **Cloud-Based Models:** Uses HuggingFace Inference API (no local dependencies)

---

## Architecture Design

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     LangGraph Supervisor Agent          â”‚
â”‚  (Coordinates all agents)                â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚          â”‚          â”‚
       â–¼          â–¼          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Image   â”‚ â”‚Classify  â”‚ â”‚  Root    â”‚
â”‚  Agent   â”‚ â”‚  Agent   â”‚ â”‚  Cause   â”‚
â”‚          â”‚ â”‚          â”‚ â”‚  Agent   â”‚
â”‚ DETR/ViT â”‚ â”‚ ViT/ML   â”‚ â”‚  LLM     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚          â”‚          â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  Report â”‚
            â”‚  Agent  â”‚
            â”‚   LLM   â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### State Schema

The system uses a `SupervisorState` (TypedDict) to pass data between agents:

```python
class SupervisorState(TypedDict):
    image_path: str                    # Path to wafer image
    wafer_id: Optional[str]           # Optional wafer identifier
    batch_id: Optional[str]            # Optional batch identifier
    defects: List[Dict]                # Detected defects (as dicts for LangGraph)
    classifications: List[Dict]          # Classification results
    root_causes: List[Dict]            # Root cause analyses
    report_path: Optional[str]          # Path to generated report
    analysis_id: str                   # Unique analysis identifier
    metadata: Dict                     # Additional metadata
    error: Optional[str]                # Error message if any
    next_agent: Optional[str]          # Next agent to execute
    agent_results: Dict[str, Any]      # Results from each agent
```

---

## Supervisor Agent

**File:** `app/agents/langgraph_supervisor.py`

### Purpose

The Supervisor Agent coordinates all sub-agents using LangGraph's StateGraph. It implements the supervisor pattern from LangChain documentation.

### Code Explanation

#### Initialization

```python
class LangGraphSupervisorAgent:
    def __init__(self):
        # Initialize all sub-agents
        self.image_agent = ImageAgent()
        self.classification_agent = ClassificationAgent()
        
        # Use advanced agents if available (LLM-based)
        if ADVANCED_AGENTS_AVAILABLE:
            self.root_cause_agent = AdvancedRootCauseAgent()
            self.report_agent = AdvancedReportAgent()
        else:
            self.root_cause_agent = RootCauseAgent()
            self.report_agent = ReportAgent()
        
        # Build LangGraph workflow
        self.workflow = self._build_workflow()
```

**Explanation:**
- Creates instances of all sub-agents
- Prefers advanced LLM-based agents if available
- Falls back to rule-based agents if LLM unavailable
- Builds the LangGraph workflow graph

#### Workflow Building

```python
def _build_workflow(self) -> StateGraph:
    workflow = StateGraph(SupervisorState)
    
    # Add nodes for each agent
    workflow.add_node("image_analysis", self._image_analysis_node)
    workflow.add_node("classification", self._classification_node)
    workflow.add_node("root_cause", self._root_cause_node)
    workflow.add_node("report_generation", self._report_generation_node)
    
    # Define edges (workflow sequence)
    workflow.set_entry_point("image_analysis")
    workflow.add_edge("image_analysis", "classification")
    workflow.add_edge("classification", "root_cause")
    workflow.add_edge("root_cause", "report_generation")
    workflow.add_edge("report_generation", END)
    
    return workflow.compile()
```

**Explanation:**
- Creates a StateGraph with SupervisorState schema
- Adds nodes for each agent's processing step
- Defines linear workflow: Image â†’ Classification â†’ Root Cause â†’ Report
- Compiles the graph for execution

#### Node Implementation

```python
def _image_analysis_node(self, state: SupervisorState) -> SupervisorState:
    """Image analysis node"""
    logger.info(f"Supervisor: Image Agent: Analyzing {state['image_path']}")
    try:
        # Call Image Agent
        defects = self.image_agent.analyze_image(state['image_path'])
        
        # Convert Pydantic models to dicts for LangGraph compatibility
        state['defects'] = [d.model_dump() if hasattr(d, 'model_dump') else d 
                           for d in defects]
        
        # Track agent results
        state['agent_results']['image_analysis'] = {
            "status": "completed",
            "message": f"Detected {len(defects)} defects"
        }
    except Exception as e:
        state['error'] = f"Image analysis failed: {e}"
        state['agent_results']['image_analysis'] = {
            "status": "failed",
            "error": str(e)
        }
    return state
```

**Explanation:**
- Each node calls the corresponding agent
- Converts Pydantic models to dictionaries (LangGraph requirement)
- Tracks success/failure in agent_results
- Returns updated state

#### Main Analysis Method

```python
def analyze_wafer_supervised(
    self,
    image_path: str,
    wafer_id: Optional[str] = None,
    batch_id: Optional[str] = None,
    metadata: Optional[Dict] = None
) -> ImageAnalysisResponse:
    """Main supervised analysis workflow using LangGraph"""
    
    # Initialize state
    initial_state: SupervisorState = {
        "image_path": image_path,
        "wafer_id": wafer_id,
        "batch_id": batch_id,
        "defects": [],
        "classifications": [],
        "root_causes": [],
        "report_path": None,
        "analysis_id": str(uuid.uuid4()),
        "metadata": metadata or {},
        "error": None,
        "next_agent": "image_analysis",
        "agent_results": {}
    }
    
    # Execute workflow
    final_state = self.workflow.invoke(initial_state, config={})
    
    # Convert back to Pydantic model for API response
    if 'analysis_response' in final_state:
        return ImageAnalysisResponse(**final_state['analysis_response'])
    else:
        # Fallback construction
        ...
```

**Explanation:**
- Creates initial state with all required fields
- Invokes the compiled workflow
- Converts final state back to Pydantic model for API response
- Handles errors gracefully

---

## Image Agent

**File:** `app/agents/image_agent.py`

### Purpose

Detects defects in wafer images using HuggingFace Inference API (DETR for object detection, ViT for feature extraction).

### Code Explanation

#### Initialization

```python
class ImageAgent:
    def __init__(self):
        self.device = settings.device
        self.confidence_threshold = settings.confidence_threshold
        
        # Login to HuggingFace
        login(token=settings.hf_api_key)
        
        # Use HuggingFace Inference API (no local models)
        self.hf_api_url = "https://router.huggingface.co/models"
        self.hf_detection_model = "facebook/detr-resnet-50"  # DETR for detection
        self.hf_classification_model = "google/vit-base-patch16-224"  # ViT for features
```

**Explanation:**
- Authenticates with HuggingFace
- Configures API endpoints
- Uses cloud-based models (no local dependencies)

#### Defect Detection via HuggingFace API

```python
def detect_defects_hf_api(self, image: np.ndarray) -> List[DefectDetection]:
    """Detect defects using HuggingFace Inference API (DETR)"""
    
    # Encode image to base64
    _, buffer = cv2.imencode('.jpg', image)
    image_base64 = base64.b64encode(buffer).decode('utf-8')
    
    # Prepare API request
    api_url = f"{self.hf_api_url}/{self.hf_detection_model}"
    headers = {
        "Authorization": f"Bearer {settings.hf_api_key}",
        "Content-Type": "application/json"
    }
    payload = {"inputs": image_base64}
    
    # Call API
    response = requests.post(api_url, headers=headers, json=payload, timeout=30)
    
    if response.status_code == 200:
        results = response.json()
        # Parse DETR results into DefectDetection objects
        defects = self._parse_detr_results(results, image.shape)
        return defects
    else:
        # Fallback to custom detection
        return self.detect_defects_custom(image)
```

**Explanation:**
- Encodes image to base64 for API transmission
- Calls HuggingFace Inference API with DETR model
- Parses DETR output (bounding boxes, scores) into DefectDetection objects
- Falls back to custom detection if API fails

#### Custom Detection (Fallback)

```python
def detect_defects_custom(self, image: np.ndarray) -> List[DefectDetection]:
    """Custom defect detection using image processing"""
    
    # Convert to grayscale
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    
    # Apply threshold
    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    
    # Find contours
    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    defects = []
    for contour in contours:
        area = cv2.contourArea(contour)
        if area > 50:  # Filter small noise
            x, y, w, h = cv2.boundingRect(contour)
            defect = DefectDetection(
                defect_id=f"defect_{len(defects)}",
                bbox=BoundingBox(x_min=x, y_min=y, x_max=x+w, y_max=y+h),
                confidence=0.7,  # Default confidence
                area=area
            )
            defects.append(defect)
    
    return defects
```

**Explanation:**
- Uses OpenCV for image processing
- Applies thresholding to detect anomalies
- Finds contours (potential defects)
- Filters by area to remove noise
- Creates DefectDetection objects

---

## Classification Agent

**File:** `app/agents/classification_agent.py`

### Purpose

Classifies detected defects into 8 categories using HuggingFace ViT model and rule-based logic.

### Code Explanation

#### Initialization

```python
class ClassificationAgent:
    def __init__(self):
        self.defect_categories = settings.defect_categories
        self._load_model()
        self.defect_characteristics = self._initialize_defect_characteristics()
    
    def _load_model(self):
        """Load classification model - using HuggingFace Inference API"""
        self.use_hf_api = True
        self.hf_api_url = "https://router.huggingface.co/models"
        logger.info("Using HuggingFace Inference API for classification")
```

**Explanation:**
- Configures HuggingFace API for classification
- Initializes defect characteristics (rules for classification)

#### Classification Method

```python
def classify_defects(self, defects: List[DefectDetection]) -> List[ClassificationResult]:
    """Classify defects into categories"""
    
    classifications = []
    for defect in defects:
        # Extract defect region from image
        defect_region = self._extract_defect_region(defect, image)
        
        # Try ML-based classification first
        ml_classification = self._classify_with_ml(defect_region)
        
        # Use rule-based classification as fallback/validation
        rule_classification = self._classify_with_rules(defect, image)
        
        # Combine results
        final_type = self._combine_classifications(ml_classification, rule_classification)
        
        classification = ClassificationResult(
            defect_id=defect.defect_id,
            defect_type=final_type,
            confidence=self._calculate_confidence(ml_classification, rule_classification),
            description=self._generate_description(final_type)
        )
        classifications.append(classification)
    
    return classifications
```

**Explanation:**
- Extracts defect region from full image
- Uses ML model (ViT) for classification
- Uses rule-based logic for validation
- Combines both approaches for final classification
- Calculates confidence score

#### ML-Based Classification

```python
def _classify_with_ml(self, defect_region: np.ndarray) -> Optional[DefectType]:
    """Classify using HuggingFace ViT model"""
    
    # Encode image
    _, buffer = cv2.imencode('.jpg', defect_region)
    image_base64 = base64.b64encode(buffer).decode('utf-8')
    
    # Call HuggingFace API
    api_url = f"{self.hf_api_url}/google/vit-base-patch16-224"
    response = requests.post(api_url, headers=headers, json={"inputs": image_base64})
    
    if response.status_code == 200:
        results = response.json()
        # Map ViT output to defect types
        return self._map_vit_to_defect_type(results)
    return None
```

**Explanation:**
- Sends defect region to ViT model via API
- Maps ViT classification output to defect types
- Returns DefectType enum

#### Rule-Based Classification

```python
def _classify_with_rules(self, defect: DefectDetection, image: np.ndarray) -> DefectType:
    """Classify using rule-based logic"""
    
    # Extract features
    area = defect.area
    aspect_ratio = (defect.bbox.x_max - defect.bbox.x_min) / (defect.bbox.y_max - defect.bbox.y_min)
    region = image[defect.bbox.y_min:defect.bbox.y_max, defect.bbox.x_min:defect.bbox.x_max]
    
    # Check against defect characteristics
    if aspect_ratio > 5 and area < 1000:
        return DefectType.SCRATCHES  # Long, thin defect
    
    if area > 5000 and self._has_pattern(region):
        return DefectType.PATTERN_DEFECTS  # Large, patterned
    
    # ... more rules
    
    return DefectType.PATTERN_DEFECTS  # Default
```

**Explanation:**
- Extracts geometric features (area, aspect ratio)
- Analyzes image region characteristics
- Matches against known defect patterns
- Returns most likely defect type

---

## Root Cause Agent

**File:** `app/agents/root_cause_agent.py` (Standard)  
**File:** `app/agents/advanced_root_cause_agent.py` (Advanced LLM-based)

### Purpose

Identifies which manufacturing process step likely caused each defect and provides recommendations.

### Code Explanation

#### Standard Root Cause Agent

```python
class RootCauseAgent:
    def __init__(self):
        self.process_steps = settings.process_steps
        self.knowledge_base = self._build_knowledge_base()
    
    def _build_knowledge_base(self) -> Dict:
        """Build knowledge base mapping defects to process steps"""
        return {
            DefectType.CMP_DEFECTS: {
                "process_step": ProcessStep.CMP,
                "likely_causes": [
                    "Insufficient polishing time",
                    "Incorrect slurry composition",
                    "Pad wear"
                ],
                "recommendations": [
                    "Check CMP tool parameters",
                    "Inspect pad condition",
                    "Review slurry quality"
                ]
            },
            # ... more mappings
        }
```

**Explanation:**
- Maps defect types to process steps
- Stores likely causes and recommendations
- Uses rule-based knowledge base

#### Advanced Root Cause Agent (LLM-Based)

```python
class AdvancedRootCauseAgent:
    def __init__(self):
        self.llm = self._initialize_llm()  # Mixtral-8x7B via HuggingFace
        self.knowledge_base = self._build_advanced_knowledge_base()
    
    def analyze_batch_advanced(
        self,
        classifications: List[ClassificationResult],
        total_defects: int,
        defect_distribution: Dict[str, int]
    ) -> List[RootCauseAnalysis]:
        """Advanced LLM-based root cause analysis"""
        
        # Build context for LLM
        context = self._build_analysis_context(classifications, defect_distribution)
        
        # Generate LLM prompt
        prompt = f"""
        Analyze these wafer defects and identify root causes:
        
        Total Defects: {total_defects}
        Defect Distribution: {defect_distribution}
        
        For each defect type, identify:
        1. Most likely process step
        2. Root cause explanation
        3. Confidence level
        4. Actionable recommendations
        
        Context: {context}
        """
        
        # Call LLM
        response = self._call_llm(prompt)
        
        # Parse LLM response
        root_causes = self._parse_llm_response(response, classifications)
        
        return root_causes
```

**Explanation:**
- Uses Mixtral-8x7B LLM for intelligent reasoning
- Builds context from defect analysis
- Generates structured prompt
- Calls HuggingFace Inference API
- Parses LLM response into RootCauseAnalysis objects

#### LLM Call Implementation

```python
def _call_llm(self, prompt: str) -> str:
    """Call HuggingFace Inference API for LLM reasoning"""
    
    api_url = "https://router.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1"
    headers = {
        "Authorization": f"Bearer {settings.hf_api_key}",
        "Content-Type": "application/json"
    }
    payload = {
        "inputs": prompt,
        "parameters": {
            "temperature": 0.3,
            "max_new_tokens": 500,
            "top_p": 0.9
        }
    }
    
    response = requests.post(api_url, headers=headers, json=payload, timeout=60)
    return response.json()[0]["generated_text"]
```

**Explanation:**
- Calls Mixtral-8x7B via HuggingFace API
- Uses low temperature (0.3) for consistent reasoning
- Limits tokens for focused responses
- Returns generated text

---

## Report Agent

**File:** `app/agents/report_agent.py` (Standard)  
**File:** `app/agents/advanced_report_agent.py` (Advanced LLM-based)

### Purpose

Generates comprehensive PDF reports with visualizations, summaries, and recommendations.

### Code Explanation

#### Standard Report Generation

```python
class ReportAgent:
    def __init__(self):
        self.reports_dir = Path("reports")
        self.reports_dir.mkdir(exist_ok=True)
        self.plots_dir = self.reports_dir / "plots"
        self.plots_dir.mkdir(exist_ok=True)
    
    def generate_report(
        self,
        analysis: ImageAnalysisResponse,
        image_path: str,
        format: str = "pdf"
    ) -> str:
        """Generate comprehensive QC report"""
        
        # Generate visualizations
        plots = self._generate_plots(analysis)
        
        # Create PDF
        report_path = self._generate_pdf_report(analysis, image_path, plots)
        
        return str(report_path)
```

**Explanation:**
- Creates reports directory structure
- Generates plots (charts, graphs)
- Creates PDF with ReportLab
- Returns report file path

#### Plot Generation

```python
def _generate_plots(self, analysis: ImageAnalysisResponse) -> Dict[str, str]:
    """Generate visualization plots"""
    
    plots = {}
    
    # Defect type distribution (pie chart)
    defect_types = list(analysis.defect_summary.keys())
    counts = list(analysis.defect_summary.values())
    
    plt.figure(figsize=(10, 6))
    plt.pie(counts, labels=defect_types, autopct='%1.1f%%')
    plt.title('Defect Type Distribution')
    pie_chart_path = self.plots_dir / f"pie_{analysis.analysis_id}.png"
    plt.savefig(pie_chart_path)
    plots['pie_chart'] = str(pie_chart_path)
    
    # Process step breakdown (bar chart)
    process_steps = [rc.process_step for rc in analysis.root_causes]
    step_counts = Counter(process_steps)
    
    plt.figure(figsize=(10, 6))
    plt.bar(step_counts.keys(), step_counts.values())
    plt.title('Defects by Process Step')
    bar_chart_path = self.plots_dir / f"bar_{analysis.analysis_id}.png"
    plt.savefig(bar_chart_path)
    plots['bar_chart'] = str(bar_chart_path)
    
    return plots
```

**Explanation:**
- Creates pie chart for defect distribution
- Creates bar chart for process step breakdown
- Saves plots as PNG files
- Returns paths to plot files

#### PDF Generation

```python
def _generate_pdf_report(
    self,
    analysis: ImageAnalysisResponse,
    image_path: str,
    plots: Dict[str, str]
) -> Path:
    """Generate PDF report using ReportLab"""
    
    report_id = f"report_{analysis.analysis_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    report_path = self.reports_dir / f"{report_id}.pdf"
    
    # Create PDF document
    doc = SimpleDocTemplate(str(report_path), pagesize=letter)
    story = []
    
    # Title
    title_style = ParagraphStyle(
        'CustomTitle',
        parent=getSampleStyleSheet()['Title'],
        fontSize=24,
        textColor=colors.HexColor('#667eea'),
        spaceAfter=30
    )
    story.append(Paragraph("Wafer Defect Analysis Report", title_style))
    story.append(Spacer(1, 12))
    
    # Executive Summary
    story.append(Paragraph("Executive Summary", styles['Heading1']))
    story.append(Paragraph(
        f"Total Defects: {analysis.total_defects}<br/>"
        f"Severity Score: {analysis.severity_score * 100:.1f}%<br/>"
        f"Defect Types: {len(analysis.defect_summary)}",
        styles['Normal']
    ))
    story.append(Spacer(1, 12))
    
    # Add plots
    for plot_name, plot_path in plots.items():
        img = Image(plot_path, width=6*inch, height=4*inch)
        story.append(img)
        story.append(Spacer(1, 12))
    
    # Detailed Analysis
    story.append(Paragraph("Detailed Analysis", styles['Heading1']))
    # ... add defect details, root causes, recommendations
    
    # Build PDF
    doc.build(story)
    
    return report_path
```

**Explanation:**
- Creates PDF document with ReportLab
- Adds title, summary, plots, and detailed analysis
- Uses professional styling
- Saves to reports directory

#### Advanced Report Agent (LLM-Based)

```python
class AdvancedReportAgent:
    def generate_advanced_report(
        self,
        analysis: ImageAnalysisResponse,
        image_path: str,
        format: str = "pdf"
    ) -> str:
        """Generate LLM-powered report with intelligent summaries"""
        
        # Generate executive summary with LLM
        summary = self._generate_llm_summary(analysis)
        
        # Generate insights with LLM
        insights = self._generate_llm_insights(analysis)
        
        # Generate recommendations with LLM
        recommendations = self._generate_llm_recommendations(analysis)
        
        # Create PDF with LLM-generated content
        report_path = self._generate_pdf_with_llm_content(
            analysis, image_path, summary, insights, recommendations
        )
        
        return str(report_path)
```

**Explanation:**
- Uses LLM to generate executive summary
- Generates intelligent insights
- Creates actionable recommendations
- Incorporates LLM content into PDF

---

## Orchestrator

**File:** `app/core/orchestrator.py`

### Purpose

Coordinates the entire workflow, initializes the Supervisor Agent, and provides the main API interface.

### Code Explanation

#### Initialization

```python
class MultiAgentOrchestrator:
    def __init__(self):
        # Initialize LangGraph Supervisor Agent
        self.supervisor = LangGraphSupervisorAgent()
        
        # Expose agents for backward compatibility
        self.image_agent = self.supervisor.image_agent
        self.classification_agent = self.supervisor.classification_agent
        self.root_cause_agent = self.supervisor.root_cause_agent
        self.report_agent = self.supervisor.report_agent
        
        # Get workflow from supervisor
        self.workflow = self.supervisor.workflow
```

**Explanation:**
- Creates Supervisor Agent instance
- Exposes sub-agents for direct access (if needed)
- Gets workflow graph from supervisor

#### Main Analysis Method

```python
def analyze_wafer(
    self,
    image_path: str,
    wafer_id: Optional[str] = None,
    batch_id: Optional[str] = None,
    metadata: Optional[Dict] = None
) -> ImageAnalysisResponse:
    """Main entry point for wafer analysis"""
    
    # Delegate to supervisor's supervised analysis
    return self.supervisor.analyze_wafer_supervised(
        image_path=image_path,
        wafer_id=wafer_id,
        batch_id=batch_id,
        metadata=metadata
    )
```

**Explanation:**
- Main API method for analysis
- Delegates to Supervisor Agent
- Returns ImageAnalysisResponse

---

## Code Flow Explanation

### Complete Workflow

1. **User uploads image** â†’ FastAPI receives request
2. **Orchestrator.analyze_wafer()** â†’ Called by API route
3. **Supervisor.analyze_wafer_supervised()** â†’ Creates initial state
4. **Workflow.invoke()** â†’ Executes LangGraph workflow
5. **Image Analysis Node** â†’ ImageAgent detects defects
6. **Classification Node** â†’ ClassificationAgent classifies defects
7. **Root Cause Node** â†’ RootCauseAgent analyzes causes
8. **Report Generation Node** â†’ ReportAgent generates PDF
9. **Final State** â†’ Converted to ImageAnalysisResponse
10. **API Response** â†’ Returned to frontend

### State Transitions

```
Initial State
    â†“
[Image Analysis] â†’ defects: []
    â†“
[Classification] â†’ classifications: []
    â†“
[Root Cause] â†’ root_causes: []
    â†“
[Report Generation] â†’ report_path: "reports/..."
    â†“
Final State â†’ ImageAnalysisResponse
```

---

## Advanced Features

### 1. LangGraph State Management

- Uses TypedDict for type safety
- Converts Pydantic models to dicts for LangGraph compatibility
- Tracks agent results and errors

### 2. Graceful Degradation

- Falls back to custom detection if API fails
- Uses rule-based agents if LLM unavailable
- Handles errors without crashing

### 3. HuggingFace Integration

- Uses Inference API (no local model dependencies)
- Supports multiple models (DETR, ViT, Mixtral)
- Handles API errors gracefully

### 4. Advanced LLM Reasoning

- Uses Mixtral-8x7B for intelligent analysis
- Generates context-aware recommendations
- Provides natural language insights

---

## Summary

This multi-agent system demonstrates:

1. **Modern Architecture:** LangGraph supervisor pattern
2. **Cloud-Based AI:** HuggingFace Inference API
3. **Advanced Reasoning:** LLM-powered analysis
4. **Production-Ready:** Error handling, logging, graceful degradation
5. **Extensible:** Easy to add new agents or modify workflow

The code is well-structured, documented, and follows best practices for multi-agent systems.

---

**Last Updated:** November 2024  
**Version:** 1.0.0

